---
title: "Sales Conversion MLOps"
publishedAt: "2025-03-06"
summary: "Explore this ML project that focuses on enhancing sales conversion rates through meticulous data handling and efficient model training."
tags: [AI, RAG, OpenAI, Pinecone, Node.js, PostgreSQL]

images:
  - "/images/projects/project-11/streamlit-prediction-app.png"
  - "/images/projects/project-11/ci-cd_pipeline_Zenml_dashboard.png
  - "/images/projects/project-11/shap_local_plot.png"
  - "/images/projects/project-11/streamlit-prediction-app.png"
  - "/images/projects/project-11/local_plot.png"
  - "/images/projects/project-11/details_interp_report.png"
team:
  - name: "Ernest Shongwe"
    role: "Software Engineer"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/ernest-shongwe/"
link: "https://github.com/bshongwe/"
---

## 🚀 Sales Conversion MLOps
This project focuses on enhancing sales conversion rates through meticulous
data handling and efficient model training. The goal is to optimize conversions
using a structured pipeline and predictive modeling.

## Overview
This project was structured to streamline the process from data ingestion and
cleaning to model training and evaluation. With an aim to empower efficient
decision-making, our pipelines incorporate quality validation tests, drift
analysis, and rigorous model performance evaluations.

It aims to streamline your sales conversion process, providing insights and
predictions to drive impactful business decisions using ZenML Integration and
Neptune Integration.

# Train Pipeline 🚂

The pipeline comprises the followin:

1. **run_pipeline.py**: Initiates the training pipeline.
2. **steps/ingest_Data**: Ingests the data, sending it to the data_validation step.
3. **data_validation step**: Conducts validation tests and transforms values.
4. **steps/clean_Data**: Carries out data preprocessing logics.
5. **data_Drift_validation step**: Conducts data drift tests.
6. **steps/train_model.py**: Utilizes h2o.ai AUTOML for model selection.
7. **src/train_models.py**: Implements the best model on the cleaned dataset.
8. **model_performance_Evaluation.py**: Assesses model performance on a split dataset.
9. **steps/alert_report.py**: If any of validation test suites doesn't meet threshold condition, email will be sent to the user, along with the failed Evidently.AI generated HTML reports.

Each step is crucial in refining and validating our model. All aboard the train pipeline! 🌟🚆

# Continuous Integration Pipeline ⚙️

The continuous integration pipeline focuses on the production environment and
streamlined processes for deployment 🔄:

1. **ci-cd.py**: Triggered to initiate the CI/CD pipeline.
2. **steps/production_batch_data**: Accesses production batch data from the Production_data folder
3. **pipelines/ci_cd_pipeline.py**: As we already discussed earlier, we conduct Data Quality, Data Drift as previously we did, if threshold fails, email reports are sent.
4. **steps/predict_production_Data.py**: Utilizes the pre-trained best model to make predictions on new production data. Then, we conduct Model Performance validation as previously we did, if threshold fails, email reports are sent.

This pipeline is crucial for maintaining a continuous and reliable deployment
process. 🔁✨

## Alert Reports 📧

Email reports are a vital part of the pipeline to notify users when certain
tests fail. These reports are triggered by specific conditions during the
pipeline execution. Here's how it works:

### E-mail Details

Upon data quality or data drift test or model performance validation tests
failures, an email is generated detailing:

- Number of total tests performed.
- Number of passed and failed tests.
- Failed test reports attached in HTML format.

# Prediction App 🚀

The Prediction App is the user-facing interface that leverages the trained models to make predictions based on user input. 🎯
To run the streamlit application,
    ```bash
    streamlit run app.py
    ```

## Functionality:
- 🌐 **Streamlit Application**: User-friendly interface for predictions and monitoring.
- 🚀 **Prediction App**: Input parameters for prediction with a link to Neptune.ai for detailed metrics.
- 📊 **Interpretability Section**: Explore detailed interpretability plots, including SHAP global and local plots.
- 📈 **Data and Model Reports**: View reports on data quality, data drift, target drift, and model performance.
- 🛠️ **Test Your Batch Data Section**: Evaluate batch data quality with 67 validation tests, receive alerts on failures.

This app streamlines the process of making predictions, interpreting model
outputs, monitoring data, and validating batch data.

## Prediction App 🚀
### User Input Data
- Fields: Impressions, Clicks, Spent, Total_Conversion, CPC.
- Predict button generates approved conversion predictions.
- 🔗 [Neptune.ai Metrics](https://app.neptune.ai/Vishal-Kumar-S/Sales-Conversion-Optimisation-MLOps-Project)




## Interpretability Section
- 📝 **Detailed Interpretability Report**: View global interpretability metrics.
- 🌐 **SHAP Global Plot**: Explore SHAP values at a global level.
- 🌍 **SHAP Local Plot**: Visualize SHAP values for user-input data.


## Data and Model Reports
- 📉 **Data Quality Report**: Assess data quality between reference and current data.
- 📊 **Data Drift Report**: Identify drift in data distribution.
- 📈 **Target Drift Report**: Monitor changes in target variable distribution.
- 📉 **Model Performance Report**: Evaluate the model's performance.

## Test Your Batch Data
1. 📂 **Dataset Upload**: Upload your batch dataset for validation.
2. 📧 **Email Alerts**: Provide an email for failure alerts.
3. 🔄 **Data Validation Progress**: 67 tests to ensure data quality.
4. 📊 **Visualizations**: Scatter plot and residuals plot for validation results.


# Neptune.ai Dashboard 🌊

## Leveraging the Power of Neptune.ai for Enhanced Insights and Management 🚀

Neptune.ai offers an intuitive dashboard for comprehensive tracking and management of experiments, model metrics, and pipeline performance.

1. **Visual Metrics**: Visualize model performance metrics with interactive charts and graphs for seamless analysis. 📈📊
2. **Experiment Management**: Track experiments, parameters, and results in a structured and organized manner. 🧪📋
3. **Integration Capabilities**: Easily integrate Neptune.ai with pipeline steps for automated tracking and reporting. 🤝🔗
4. **Collaboration Tools**: Facilitate teamwork with collaborative features and easy sharing of experiment results. 🤝💬
5. **Code and Environment Tracking**: Monitor code versions and track environments used during experimentation for reproducibility. 🛠️📦


## CML Reports Integration 🚀

🎯 Predictions Scatter Plot: Visualizes model predictions against actual conversions.
📈 Residuals Plot: Illustrates the differences between predicted and actual values.

## GitHub Actions Workflow 🛠️

Integrated into CI/CD pipeline:
- Automatic generation on every push event.
- Visual insights available directly in the repository.